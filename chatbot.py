import os
import json
import time  # Pour time.sleep
import requests
import streamlit as st
from streamlit_lottie import st_lottie
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain.chains import RetrievalQA
from langchain.vectorstores import Chroma
from datetime import datetime
import plotly.express as px
from sklearn.decomposition import PCA
import numpy as np
import pandas as pd
import re

# D√©sactivez le mode multi‚Äëtenant d√®s le d√©but
os.environ["CHROMADB_DISABLE_MULTITENANT"] = "true"

# Configuration de la page
st.set_page_config(page_title="Chatbot Historique ‚öîÔ∏è", page_icon="‚öîÔ∏è", layout="centered")

# √âtape 1 : Demander l'acceptation des CGV et de la politique de confidentialit√©
# Ajouter la case pour accepter les conditions g√©n√©rales d'utilisation (CGV)
accept_cgv = st.checkbox("J'accepte les conditions g√©n√©rales de vente et la politique de confidentialit√©.")
if not accept_cgv:
    st.warning("Vous devez accepter les conditions g√©n√©rales de vente pour interagir avec le chatbot.")
    st.stop()

# Lien vers la politique de confidentialit√©
st.markdown("[Politique de confidentialit√©](URL_de_votre_politique_de_confidentialit√©)", unsafe_allow_html=True)

# Styles CSS personnalis√©s pour simuler un chat de type WhatsApp avec des couleurs plus fonc√©es
st.markdown(
    """
    <style>
        body {
            background-color: #f3f4f6;
            font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif;
        }
        .stButton>button {
            color: white;
            background-color: #4CAF50;
            border-radius: 10px;
            font-size: 18px;
            padding: 10px 20px;
            border: none;
        }
        .stTextInput>div>div>input {
            background-color: #ffffff;
            color: black;
            font-size: 16px;
            padding: 8px;
        }
        .container {
            max-width: 700px;
            margin: auto;
            padding: 20px;
            background: white;
            border-radius: 10px;
            box-shadow: 0px 4px 10px rgba(0, 0, 0, 0.1);
            text-align: center;
        }
        .header {
            text-align: center;
            padding: 20px;
        }
        .lottie-container {
            display: flex;
            justify-content: center;
            align-items: center;
        }
        /* Ajout du style pour les messages avec des couleurs plus fonc√©es */
        .user-message {
            background-color: #0066cc;  /* Bleu plus fonc√© */
            color: white;
            padding: 10px;
            margin: 5px 0;
            border-radius: 10px;
            text-align: left;
            width: fit-content;
            max-width: 80%;
            display: inline-block;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
        }
        .bot-message {
            background-color: #28a745;  /* Vert plus fonc√© */
            color: white;
            padding: 10px;
            margin: 5px 0;
            border-radius: 10px;
            text-align: left;
            width: fit-content;
            max-width: 80%;
            display: inline-block;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
        }
    </style>
    """,
    unsafe_allow_html=True
)

# Chargement de l'animation depuis le fichier JSON
with open("soldiers_animation.json", "r") as f:
    lottie_soldiers = json.load(f)

# V√©rification de la cl√© API OpenAI
openai_api_key = os.getenv("OPENAI_API_KEY")
if not openai_api_key:
    st.error("üö® Cl√© API OpenAI non trouv√©e.")
    st.stop()

# Chargement des donn√©es JSON contenant les √©v√©nements historiques
with open("combined_data.json", "r", encoding="utf-8") as f:
    data = json.load(f)

# Filtrer les documents entre 1914 et 1945
def filter_documents_by_date(documents, start_year=1918, end_year=1945):
    """
    Filtrer les documents pour inclure uniquement ceux entre 1918 et 1945.
    Ajoute une gestion d'erreurs pour les dates mal format√©es et les dates sous format texte.
    """
    filtered_docs = []
    
    for doc in documents:
        date_str = doc['date']  # Exemple : "18 d√©cembre 1917"
        
        # Expression r√©guli√®re pour extraire l'ann√©e √† partir de la cha√Æne de texte
        match = re.search(r'\b(\d{4})\b', date_str)  # Recherche un nombre √† 4 chiffres (ann√©e)
        
        if match:
            year = int(match.group(1))  # Extraire l'ann√©e du match trouv√©
            if start_year <= year <= end_year:
                filtered_docs.append(doc)
        else:
            # Si aucune ann√©e n'est trouv√©e, on peut l'ignorer ou g√©rer l'erreur
            print(f"Date mal format√©e ou sans ann√©e : {date_str}")
    
    return filtered_docs

# Filtrer les documents par p√©riode (1918-1945)
filtered_data = filter_documents_by_date(data)

# Filtrer les doublons et transformer les entr√©es en documents LangChain
docs = []
for entry in filtered_data:
    if "duplicate" in entry and not entry["duplicate"]:
        text = f"{entry['date']} : {entry['event']}"
        docs.append(Document(page_content=text))

# D√©coupage des documents en morceaux
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
split_docs = text_splitter.split_documents(docs)

# Initialisation (ou chargement) de la base vectorielle Chroma
persist_directory = "chroma_db"
embedding = OpenAIEmbeddings(openai_api_key=openai_api_key)

# V√©rification de l'existence de l'index et rechargement ou recr√©ation de l'index
if os.path.exists(persist_directory) and os.listdir(persist_directory):
    # Si l'index existe, on le charge
    vector_store = Chroma(
        persist_directory=persist_directory,
        embedding_function=embedding
    )
else:
    # Si l'index n'existe pas, on le recr√©e
    vector_store = Chroma.from_documents(
        documents=split_docs,
        embedding=embedding,
        persist_directory=persist_directory
    )
    vector_store.persist()

# Cr√©ation du retriever pour interroger la base de donn√©es
retriever = vector_store.as_retriever()

# Configuration du mod√®le OpenAI via LangChain
llm = ChatOpenAI(
    model="gpt-3.5-turbo",
    openai_api_key=openai_api_key,
    temperature=0.0
)

# Construction de la cha√Æne de questions-r√©ponses
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
)

# --- BOUTON D'ACTUALISATION DANS L'OVERLAY GAUCHE ---
def my_rerun():
    if hasattr(st, "experimental_rerun"):
        st.experimental_rerun()
    else:
        st.warning("La fonction de rafra√Æchissement n'est pas support√©e dans votre version de Streamlit. Veuillez mettre √† jour Streamlit (pip install --upgrade streamlit).")

st.sidebar.button("Actualiser l'historique", on_click=my_rerun)

# --- MENU LAT√âRAL ---
st.sidebar.header("Instructions")
st.sidebar.info(
    "Posez vos questions sur l'histoire de France via le formulaire principal. "
    "L'historique des interactions est affich√© ci-dessous."
)

with st.sidebar.expander("Historique des interactions r√©cents"):
    with st.spinner("Chargement de l'historique..."):
        results = vector_store.similarity_search("R√©ponse", k=5)
        if results:
            for doc in results:
                st.markdown(doc.page_content)
                st.markdown("---")
        else:
            st.info("Aucune interaction trouv√©e.")

st.sidebar.markdown("### √Ä propos")
st.sidebar.info(
    "Ce chatbot utilise LangChain et l'API OpenAI pour r√©pondre √† vos questions sur l'histoire de France. "
    "Les interactions sont sauvegard√©es et affich√©es dans l'historique."
)

# --- ZONE PRINCIPALE ---
with st.container():
    st.markdown('<div class="header"><h1>‚öîÔ∏è Chatbot Historique ‚öîÔ∏è</h1></div>', unsafe_allow_html=True)
    st_lottie(lottie_soldiers, speed=1, width=400, height=300, key="soldiers")
    
    
    # Liste des mots-cl√©s √† filtrer pour interdire des sujets modernes
forbidden_keywords = ["ma√Ætre gims", "trello", "Steve jobs", "napol√©on", "r√©volution fran√ßaise", "1789", "rois de france"]


def get_recent_history(vector_store, max_tokens=4096):
    """
    R√©cup√®re l'historique des messages et tronque le contexte pour ne pas d√©passer la limite de jetons.
    """
    results = vector_store.similarity_search("R√©ponse", k=5)  # Ajustez le nombre de documents r√©cents √† r√©cup√©rer
    history = ""
    
    # Concat√©ner les r√©sultats jusqu'√† ce que la longueur du contexte d√©passe la limite des jetons
    for result in results[::-1]:  # On commence par les documents les plus anciens
        history += result.page_content + "\n"
        
        # V√©rification de la longueur actuelle du contexte
        if len(history.split()) > max_tokens:
            break
    
    return history


# Fonction pour interroger le chatbot et enregistrer l'historique
def chat_with_bot(query):
    # V√©rification des mots-cl√©s interdits
    if any(keyword in query.lower() for keyword in forbidden_keywords):
        return "D√©sol√©, je ne peux r√©pondre qu'aux questions concernant les guerres mondiales (1914-1945)."
    
    # V√©rification d'une date valide dans la question
    date_match = re.search(r'\b(\d{1,2})\s([a-zA-Z√©√†√ª]+)\s(\d{4})\b', query)  # D√©tecter les dates dans le format "18 juin 1917"
    if date_match:
        year = int(date_match.group(3))
        if year < 1918 or year > 1945:
            return "D√©sol√©, je ne peux r√©pondre qu'aux √©v√©nements entre 1918 et 1945."
    else:
        return "D√©sol√©, je ne peux r√©pondre qu'aux questions sur les √©v√©nements entre 1918 et 1945."
    
    # R√©cup√©rer l'historique r√©cent et tronquer le contexte si n√©cessaire
    recent_history = get_recent_history(vector_store, max_tokens=4096)  # Utiliser 4096 jetons pour le contexte
    
    # Cr√©er un prompt avec la question actuelle et l'historique r√©duit
    prompt = recent_history + f"\nQuestion: {query}\nR√©ponse:"
    
    # Interroger le mod√®le OpenAI via LangChain
    response = qa_chain.run(prompt)
    time.sleep(1)  # Pause d'1 seconde pour limiter le risque de blocage par l'API OpenAI

    # Ajout d'un timestamp pour chaque interaction
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    history_entry = f"**[{timestamp}] Question:** {query}\n\n**R√©ponse:** {response}"
    history_doc = Document(page_content=history_entry)

    # Ajout de l'entr√©e d'historique √† Chroma et sauvegarde
    vector_store.add_documents([history_doc])
    vector_store.persist()

    return response



# Formulaire pour saisir la question avec bouton centr√©
with st.form(key="chat_form", clear_on_submit=True):
    query = st.text_input("Posez votre question sur l'histoire de France :", "")
    col1, col2, col3 = st.columns([1, 1, 1])
    submit_button = col2.form_submit_button("Envoyer")

if submit_button:
    if query:
        with st.spinner("Recherche..."):
            response = chat_with_bot(query)
        # Affichage conversationnel avec des bulles distinctes pour l'utilisateur et le bot
        st.markdown(f'<div class="user-message">{query}</div>', unsafe_allow_html=True)
        st.markdown(f'<div class="bot-message">{response}</div>', unsafe_allow_html=True)
    else:
        st.warning("Veuillez entrer une question.")

# Option de t√©l√©chargement de l'historique complet dans la sidebar
with st.sidebar.expander("T√©l√©charger l'historique complet"):
    with st.spinner("Pr√©paration du fichier..."):
        results_all = vector_store.similarity_search("R√©ponse", k=100)
        history_text = "\n\n".join([doc.page_content for doc in results_all])
    st.download_button("T√©l√©charger l'historique", history_text, "historique.txt", "text/plain")
