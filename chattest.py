import json
import os
import time
import openai
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.schema import Document
from langchain.chains import RetrievalQA
from tqdm import tqdm
from openai import OpenAIError

# üìÇ Nom du fichier pour sauvegarder l'historique
HISTORY_FILE = "chat_history.json"

# üìå Charger l'historique des conversations si le fichier existe
def load_chat_history():
    if os.path.exists(HISTORY_FILE):
        with open(HISTORY_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return []

# üìå Sauvegarder l'historique mis √† jour
def save_chat_history(history):
    with open(HISTORY_FILE, "w", encoding="utf-8") as f:
        json.dump(history, f, indent=4, ensure_ascii=False)

# üìå Initialiser l'historique
chat_history = load_chat_history()

# 1Ô∏è‚É£ V√©rifier la cl√© API d'OpenAI
if "OPENAI_API_KEY" not in os.environ or not os.environ["OPENAI_API_KEY"]:
    print("‚ùå Erreur : Cl√© API OpenAI manquante. D√©finissez la variable d'environnement OPENAI_API_KEY.")
    exit()

# 2Ô∏è‚É£ Charger les donn√©es du JSON
try:
    with open("filtered_output.json", "r", encoding="utf-8") as f:
        data = json.load(f)
except FileNotFoundError:
    print("‚ùå Erreur : Fichier JSON introuvable.")
    exit()

# 3Ô∏è‚É£ Filtrer les doublons et transformer en documents LangChain avec affichage de progression
docs = []
for entry in tqdm(data, desc="Indexation des √©v√©nements..."):
    if not entry.get("duplicate", False):
        text = f"{entry.get('date', 'Date inconnue')} : {entry.get('event', '√âv√©nement non sp√©cifi√©')}"
        docs.append(Document(page_content=text))

# 4Ô∏è‚É£ Fractionner les textes pour l'indexation
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
split_docs = text_splitter.split_documents(docs)

# 5Ô∏è‚É£ V√©rifier si FAISS est bien install√©
try:
    import faiss
except ModuleNotFoundError:
    print("‚ùå Erreur : FAISS n'est pas install√©. Ex√©cutez `pip install faiss-cpu` ou `pip install faiss-gpu`.")
    exit()

# 6Ô∏è‚É£ V√©rifier si l'index FAISS existe d√©j√†
index_path = "faiss_index"
if os.path.exists(index_path):
    print("üîÑ Chargement de l'index FAISS existant...")
    try:
        vector_store = FAISS.load_local(index_path, OpenAIEmbeddings(), allow_dangerous_deserialization=True)
    except Exception as e:
        print(f"‚ùå Erreur lors du chargement de FAISS : {e}")
        exit()
else:
    print("‚ö° Aucun index trouv√©, cr√©ation d'un nouvel index...")
    vector_store = FAISS.from_documents(split_docs, OpenAIEmbeddings())
    vector_store.save_local(index_path)

# 7Ô∏è‚É£ Cr√©er un retriever pour interroger les donn√©es historiques
retriever = vector_store.as_retriever()

# 8Ô∏è‚É£ Configurer le mod√®le OpenAI via LangChain
try:
    llm = ChatOpenAI(model="gpt-3.5-turbo")  # ou "gpt-4"
except OpenAIError as e:
    print(f"‚ùå Erreur de configuration OpenAI : {e}")
    exit()

# 9Ô∏è‚É£ Construire la cha√Æne de questions-r√©ponses
qa_chain = RetrievalQA.from_chain_type(llm, retriever=retriever)

# üîü Fonction pour interroger le chatbot avec gestion de l'historique
def chat_with_bot(query, max_retries=3, wait_time=5):
    """
    Interroge le chatbot avec gestion du rate limit et sauvegarde des conversations.
    """
    for attempt in range(max_retries):
        try:
            response = qa_chain.invoke({"query": query})  
            time.sleep(1)  # ‚úÖ Ajout d'une pause pour limiter la fr√©quence des requ√™tes
            answer = response.get("result", "‚ö†Ô∏è Aucune r√©ponse disponible.")

            # üìå Ajouter la conversation √† l'historique
            chat_history.append({"question": query, "response": answer})
            save_chat_history(chat_history)  # Sauvegarde apr√®s chaque interaction

            return answer
        except openai.RateLimitError:
            print(f"‚ö†Ô∏è API Rate Limit atteinte. Attente de {wait_time} secondes avant de r√©essayer...")
            time.sleep(wait_time)  
        except OpenAIError as e:
            print(f"‚ùå Erreur OpenAI : {e}")
            return "‚ö†Ô∏è Une erreur est survenue avec l'API OpenAI."
        except Exception as e:
            print(f"‚ùå Erreur inattendue : {e}")
            return "‚ö†Ô∏è Une erreur inattendue est survenue."

    return "‚ö†Ô∏è D√©sol√©, trop de requ√™tes envoy√©es √† OpenAI. R√©essayez plus tard."

# üî• Boucle interactive pour discuter avec le chatbot
print("\nü§ñ Chatbot Historique - Tapez 'exit' ou 'quit' pour quitter.\n")

# üìå Afficher l'historique si disponible
if chat_history:
    print("üìú Historique des conversations pr√©c√©dentes :")
    for i, conv in enumerate(chat_history[-5:], start=1):  # Affiche les 5 derni√®res
        print(f"{i}. Vous: {conv['question']}")
        print(f"   Bot: {conv['response']}\n")

while True:
    user_input = input("Vous: ")
    
    if user_input.lower() in ["exit", "quit"]:
        print("üëã Merci d'avoir utilis√© le chatbot historique ! √Ä bient√¥t.")
        break
    
    bot_response = chat_with_bot(user_input)
    print(f"Bot: {bot_response}\n")
